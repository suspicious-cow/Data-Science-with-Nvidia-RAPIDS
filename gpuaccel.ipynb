{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Accelerating Data Science Workflows with RAPIDS and cuDF\n",
    "\n",
    "This notebook demonstrates how to transition from traditional CPU-based data processing to GPU-accelerated workflows using **RAPIDS**, an open-source suite of libraries developed by NVIDIA.\n",
    "\n",
    "## Overview\n",
    "\n",
    "GPU acceleration is a game-changer for handling large datasets and complex operations. RAPIDS provides tools like **cuDF** for pandas-like GPU-accelerated data manipulation, enabling faster and more efficient workflows with minimal code changes.\n",
    "\n",
    "## Key Steps in the Notebook\n",
    "\n",
    "### 1. Setting Up RAPIDS\n",
    "The notebook begins by setting up RAPIDS. This involves enabling the cuDF extension, which allows you to run familiar pandas-like operations on GPUs seamlessly. \n",
    "\n",
    "### 2. Data Loading and Preparation\n",
    "- **Data Ingestion**: Data is loaded into pandas DataFrames from CSV files.\n",
    "- **Dataset Expansion**: The dataset is scaled to simulate large workloads by duplicating rows to reach a target size of 1 million rows.\n",
    "\n",
    "### 3. GPU-Accelerated Data Manipulation\n",
    "Using cuDF, common data manipulation tasks such as filtering, grouping, and merging are performed:\n",
    "- **Filtering**: Subset rows based on conditions.\n",
    "- **Grouping**: Aggregate data, e.g., calculating averages by group.\n",
    "- **Merging**: Combine multiple DataFrames with additional metadata. \n",
    "\n",
    "### 4. Performance Profiling and Benchmarking\n",
    "The notebook compares the execution time of key operations on the CPU and GPU:\n",
    "- **Profiling**: Tools like `%cudf.pandas.profile` provide detailed performance metrics for GPU operations.\n",
    "- **Benchmarking**: Commands like `%%time` and `%%timeit` measure and compare CPU and GPU runtimes, highlighting significant speedups achieved with GPU acceleration.\n",
    "\n",
    "### 5. Verifying GPU Utilization\n",
    "Checks are performed to confirm that operations are leveraging GPU resources effectively:\n",
    "- **Type Checks**: Ensure arrays and DataFrames are processed on the GPU.\n",
    "- **Execution Paths**: Verify whether the cuDF accelerator is active or falling back to pandas.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "By using RAPIDS and cuDF, this notebook demonstrates how to achieve significant performance improvements in data manipulation workflows. GPU acceleration enables faster processing for large datasets while maintaining compatibility with familiar pandas-like syntax, making it an accessible and powerful tool for data scientists.\n",
    "\n",
    "This workflow illustrates the potential of RAPIDS to enhance efficiency and scalability in data science pipelines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# -------------------------\n",
    "# Step 1: Load Data with CPU pandas\n",
    "# -------------------------\n",
    "train_df = pd.read_csv('./Titanic-Train-Dataset.csv') \n",
    "test_df = pd.read_csv('./Titanic-Test-Dataset.csv') \n",
    "\n",
    "# -------------------------\n",
    "# Step 2: Expand DataFrames to ~1,000,000 Rows (if needed)\n",
    "# -------------------------\n",
    "target_rows = 1_000_000\n",
    "# Use ceiling division to determine repeats\n",
    "repeats_train = -(-target_rows // len(train_df))\n",
    "repeats_test  = -(-target_rows // len(test_df))\n",
    "\n",
    "# Concatenate copies to reach the desired number of rows\n",
    "train_df = pd.concat([train_df] * repeats_train, ignore_index=True).head(target_rows)\n",
    "test_df  = pd.concat([test_df] * repeats_test, ignore_index=True).head(target_rows)\n",
    "\n",
    "# Create a combined list to mimic your workflow\n",
    "combine = [train_df, test_df]\n",
    "\n",
    "print(\"Before:\", train_df.shape, test_df.shape, combine[0].shape, combine[1].shape)\n",
    "\n",
    "# -------------------------\n",
    "# Step 3: Time the Operation Using Python's Time Module\n",
    "# -------------------------\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "# Drop the 'Ticket' and 'Cabin' columns using CPU pandas\n",
    "train_df_trunc = train_df.drop(['Ticket', 'Cabin'], axis=1)\n",
    "test_df_trunc  = test_df.drop(['Ticket', 'Cabin'], axis=1)\n",
    "combine = [train_df_trunc, test_df_trunc]\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "\n",
    "# Convert the elapsed time to milliseconds\n",
    "elapsed_ms = (end_time - start_time) * 1000\n",
    "\n",
    "print(\"After:\", train_df_trunc.shape, test_df_trunc.shape, combine[0].shape, combine[1].shape)\n",
    "print(f\"Elapsed time: {elapsed_ms:.4f} milliseconds\")\n",
    "\n",
    "# -------------------------\n",
    "# Step 4: Save final elapsed time for comparison later\n",
    "# -------------------------\n",
    "final_elapsed_time_ms = (end_time - start_time) * 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------------\n",
    "# The following line is a Jupyter Notebook magic command that loads the cuDF pandas\n",
    "# extension. This extension allows you to work with GPU-accelerated DataFrames using\n",
    "# a pandas-like API.\n",
    "#\n",
    "# Note:\n",
    "#   - This command is only valid within a Jupyter Notebook environment.\n",
    "#   - If you attempt to run this code in a standard Python script, it will raise a\n",
    "#     SyntaxError because the \"%load_ext\" syntax is not recognized in regular Python.\n",
    "# ------------------------------------------------------------------------------------\n",
    "\n",
    "%load_ext cudf.pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script loads the Titanic training and testing datasets using pandas,\n",
    "concatenates them vertically, and prepares the data for further analysis.\n",
    "\n",
    "Note:\n",
    "    - The `cupy` module is imported to support GPU-accelerated operations if needed.\n",
    "      In this snippet, it is not actively used.\n",
    "\"\"\"\n",
    "\n",
    "# Import the pandas library for data manipulation and analysis.\n",
    "import pandas as pd\n",
    "\n",
    "# Import the cupy library for GPU-accelerated numerical computations.\n",
    "import cupy as cp\n",
    "\n",
    "# -------------------------\n",
    "# Step 1: Load Datasets\n",
    "# -------------------------\n",
    "# Load the Titanic training dataset from the CSV file.\n",
    "train = pd.read_csv('./Titanic-Train-Dataset.csv')\n",
    "\n",
    "# Load the Titanic testing dataset from the CSV file.\n",
    "test = pd.read_csv('./Titanic-Test-Dataset.csv')\n",
    "\n",
    "# -------------------------\n",
    "# Step 2: Concatenate DataFrames\n",
    "# -------------------------\n",
    "# Concatenate the training and testing DataFrames vertically.\n",
    "# The axis=0 parameter indicates vertical concatenation.\n",
    "concat = pd.concat([train, test], axis=0)\n",
    "\n",
    "# Optionally, print the shape of the concatenated DataFrame to verify the result.\n",
    "print(\"Concatenated DataFrame shape:\", concat.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Expand the train and test DataFrames to have exactly 1,000,000 rows each by repeating\n",
    "their rows. The script uses ceiling division to determine the number of repeats needed,\n",
    "concatenates the repeated DataFrame copies, and then truncates to the target number of rows.\n",
    "Finally, the expanded DataFrames are combined into a list for further processing.\n",
    "\n",
    "Expected outcomes (depending on the original column counts):\n",
    "    - The expanded training DataFrame might have a shape like (1000000, N_train)\n",
    "    - The expanded testing DataFrame might have a shape like (1000000, N_test)\n",
    "\n",
    "In your example, commented shapes were:\n",
    "    - train_df: (1000000, 2)\n",
    "    - test_df:  (1000000, 2)\n",
    "    - Later output showing (1000000, 12) and (1000000, 11) suggest differing original column counts.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Define the target number of rows for the expanded DataFrames.\n",
    "TARGET_ROWS = 1_000_000\n",
    "\n",
    "# -------------------------\n",
    "# Expand the Training DataFrame\n",
    "# -------------------------\n",
    "# Calculate the number of repeats required using ceiling division.\n",
    "# The formula: -(-TARGET_ROWS // len(train)) ensures that even if TARGET_ROWS is not an\n",
    "# exact multiple of len(train), the result rounds up.\n",
    "repeats = -(-TARGET_ROWS // len(train))\n",
    "\n",
    "# Concatenate copies of the training DataFrame to reach or exceed TARGET_ROWS.\n",
    "# The ignore_index=True parameter resets the index in the concatenated DataFrame.\n",
    "# Finally, .head(TARGET_ROWS) ensures that the resulting DataFrame contains exactly TARGET_ROWS rows.\n",
    "train_df = pd.concat([train] * repeats, ignore_index=True).head(TARGET_ROWS)\n",
    "\n",
    "# Print the shape of the expanded training DataFrame.\n",
    "# The comment indicates an expected shape, but actual results depend on the original DataFrame.\n",
    "print(\"Training DataFrame shape:\", train_df.shape)  # e.g., (1000000, 2) or (1000000, 12)\n",
    "\n",
    "# -------------------------\n",
    "# Expand the Testing DataFrame\n",
    "# -------------------------\n",
    "# Calculate the number of repeats needed for the test DataFrame using the same ceiling division.\n",
    "repeats = -(-TARGET_ROWS // len(test))\n",
    "\n",
    "# Expand the test DataFrame in the same manner.\n",
    "test_df = pd.concat([test] * repeats, ignore_index=True).head(TARGET_ROWS)\n",
    "\n",
    "# Print the shape of the expanded testing DataFrame.\n",
    "print(\"Testing DataFrame shape:\", test_df.shape)  # e.g., (1000000, 2) or (1000000, 11)\n",
    "\n",
    "# -------------------------\n",
    "# Combine the Expanded DataFrames\n",
    "# -------------------------\n",
    "# Place the expanded training and testing DataFrames into a list for further processing.\n",
    "combine = [train_df, test_df]\n",
    "\n",
    "# Print the shapes of the combined DataFrames for verification.\n",
    "# Note: The shapes may vary if the original 'train' and 'test' DataFrames have different numbers\n",
    "#       of columns. For example:\n",
    "#       - combine[0].shape might be (1000000, 12)\n",
    "#       - combine[1].shape might be (1000000, 11)\n",
    "print(\"Combined DataFrame shapes:\")\n",
    "print(\"Train part shape:\", combine[0].shape)\n",
    "print(\"Test part shape:\", combine[1].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Print the column names of the expanded training and testing DataFrames.\n",
    "\n",
    "This snippet assumes that the DataFrames `train_df` and `test_df` have been\n",
    "previously defined (for example, by loading CSV data and expanding them as shown\n",
    "in previous code snippets). The output will display the column labels for each\n",
    "DataFrame, helping to verify that the data has been loaded and processed correctly.\n",
    "\"\"\"\n",
    "\n",
    "# Print the column names for the training DataFrame.\n",
    "print(\"Training DataFrame columns:\", train_df.columns)\n",
    "\n",
    "# Print the column names for the testing DataFrame.\n",
    "print(\"Testing DataFrame columns:\", test_df.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cudf.pandas.profile\n",
    "# ------------------------------------------------------------------------------------\n",
    "# This cell profiles a sequence of DataFrame operations on 'train_df' using the\n",
    "# cuDF pandas extension. The operations include:\n",
    "#\n",
    "#   1. Selecting the 'Pclass' and 'Survived' columns from the DataFrame.\n",
    "#   2. Grouping the data by 'Pclass'. The parameter `as_index=False` ensures that\n",
    "#      'Pclass' remains a column in the resulting DataFrame rather than becoming the index.\n",
    "#   3. Calculating the mean for each group (i.e., the mean 'Survived' rate per 'Pclass').\n",
    "#   4. Sorting the resulting DataFrame by the 'Survived' column in descending order.\n",
    "#\n",
    "# These operations allow you to quickly understand how survival rates vary across\n",
    "# different passenger classes on the Titanic.\n",
    "# ------------------------------------------------------------------------------------\n",
    "\n",
    "# Select the columns 'Pclass' and 'Survived', then group by 'Pclass' without setting\n",
    "# 'Pclass' as the index. Compute the mean values for each group and sort the result\n",
    "# by the 'Survived' column in descending order.\n",
    "train_df[[\"Pclass\", \"Survived\"]].groupby(\n",
    "    [\"Pclass\"], as_index=False\n",
    ").mean().sort_values(\n",
    "    by=\"Survived\", ascending=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script compares GPU and CPU execution times for specific DataFrame operations.\n",
    "It assumes that:\n",
    "    - A CPU runtime (in milliseconds) has already been measured and stored in\n",
    "      'final_elapsed_time_ms'.\n",
    "    - The DataFrames 'train_df', 'test_df', and the list 'combine' have been defined.\n",
    "    \n",
    "The GPU operation involves dropping the 'Ticket' and 'Cabin' columns from both the\n",
    "training and testing DataFrames using GPU-accelerated operations. The script times\n",
    "this operation and then prints a table comparing the CPU and GPU runtimes.\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "\n",
    "# Assume the CPU runtime (in milliseconds) has been measured in a previous cell or block.\n",
    "cpu_elapsed_ms = final_elapsed_time_ms  # Previously recorded CPU runtime\n",
    "\n",
    "# -------------------------\n",
    "# GPU-Accelerated Operation Timing\n",
    "# -------------------------\n",
    "# Record the start time for the GPU operation.\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "# Print the shapes of the DataFrames before performing the GPU-accelerated operation.\n",
    "print(\"Before\", train_df.shape, test_df.shape, combine[0].shape, combine[1].shape)\n",
    "\n",
    "# Drop the 'Ticket' and 'Cabin' columns from the training DataFrame.\n",
    "train_df_trunc = train_df.drop(['Ticket', 'Cabin'], axis=1)\n",
    "\n",
    "# Drop the 'Ticket' and 'Cabin' columns from the testing DataFrame.\n",
    "test_df_trunc = test_df.drop(['Ticket', 'Cabin'], axis=1)\n",
    "\n",
    "# Update the combined list to include the truncated DataFrames.\n",
    "combine = [train_df_trunc, test_df_trunc]\n",
    "\n",
    "# Print the shapes of the DataFrames after dropping the specified columns.\n",
    "print(\"After\", train_df_trunc.shape, test_df_trunc.shape, combine[0].shape, combine[1].shape)\n",
    "\n",
    "# Record the end time for the GPU operation.\n",
    "end_time = time.perf_counter()\n",
    "\n",
    "# Calculate the GPU runtime in milliseconds.\n",
    "gpu_elapsed_ms = (end_time - start_time) * 1000\n",
    "\n",
    "# Print the GPU runtime with four decimal precision.\n",
    "print(f\"GPU runtime: {gpu_elapsed_ms:.4f} milliseconds\")\n",
    "\n",
    "# -------------------------\n",
    "# Compare the CPU and GPU runtimes and display the results in a table\n",
    "# -------------------------\n",
    "# Calculate the time savings by subtracting the GPU runtime from the CPU runtime.\n",
    "time_savings_ms = cpu_elapsed_ms - gpu_elapsed_ms\n",
    "\n",
    "# Print a table to compare the CPU and GPU runtimes.\n",
    "print(\"\\nTime Performance Comparison:\")\n",
    "print(\"+----------------+---------------+\")\n",
    "print(\"| Metric         | Time (ms)     |\")\n",
    "print(\"+----------------+---------------+\")\n",
    "print(f\"| CPU Runtime    | {cpu_elapsed_ms:13.4f} |\")\n",
    "print(f\"| GPU Runtime    | {gpu_elapsed_ms:13.4f} |\")\n",
    "print(\"+----------------+---------------+\")\n",
    "print(f\"| Savings        | {time_savings_ms:13.4f} |\")\n",
    "print(\"+----------------+---------------+\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Display DataFrame Information\n",
    "\n",
    "This snippet prints summary information about the 'train_df' DataFrame.\n",
    "The information includes:\n",
    "    - The number of non-null entries per column.\n",
    "    - The data type of each column.\n",
    "    - Memory usage details.\n",
    "    \n",
    "This is useful for quickly assessing the structure and integrity of the dataset.\n",
    "\n",
    "Assumptions:\n",
    "    - The 'train_df' DataFrame has already been loaded (for example, via pd.read_csv()).\n",
    "\"\"\"\n",
    "\n",
    "# Print detailed information about the train_df DataFrame.\n",
    "train_df.info()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-24.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
